# data-pipeline
```
A modular, production-ready Question & Answer (Q/A) generation pipeline designed for bank policy documents.
The project transforms large policy PDFs into validated Q/A pairs using LLMs (Qwen 7B), evidence extraction, and semantic deduplication.

---
```

## ğŸš€ Features

```
- PDF/TXT text extraction
- Advanced cleaning for policy documents
- Configurable chunking with overlap
- Qwen 7B (4-bit) model support via `bitsandbytes`
- Deterministic + sampling generation retries
- Strict question validation (section numbers, policy names, acronyms)
- Q/A parsing with numbering pattern matching
- Sentence-level evidence extraction using `sentence-transformers`
- Exact + semantic deduplication (FAISS cosine similarity)
- Fully modular source code
- CLI interface for terminal execution

---
```

## ğŸ“ Project Structure

```
data-pipeline/
â”œâ”€â”€ README.md
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ pipeline_config.yaml
â”‚   â””â”€â”€ model_config.yaml
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ input/
â”‚   â”‚   â””â”€â”€ (place input PDF/TXT here)
â”‚   â”œâ”€â”€ interim/
â”‚   â””â”€â”€ output/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ cli.py
â”‚   â”œâ”€â”€ pipeline_runner.py
â”‚   â”œâ”€â”€ file_loader.py
â”‚   â”œâ”€â”€ cleaner.py
â”‚   â”œâ”€â”€ chunker.py
â”‚   â”œâ”€â”€ prompts.py
â”‚   â”œâ”€â”€ model_loader.py
â”‚   â”œâ”€â”€ generator.py
â”‚   â”œâ”€â”€ qa_parser.py
â”‚   â”œâ”€â”€ validators.py
â”‚   â”œâ”€â”€ evidence.py
â”‚   â””â”€â”€ dedupe.py
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ experiments.ipynb
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_cleaner.py
â”‚   â”œâ”€â”€ test_chunker.py
â”‚   â””â”€â”€ test_parser.py
â””â”€â”€ docs/
    â””â”€â”€ architecture.md


````

---
```
## ğŸ›  Installation

### 1. Create virtual environment
```bash
python3 -m venv .venv
source .venv/bin/activate       # macOS / Linux
# or
.venv\Scripts\activate          # Windows
````

### 2. Install dependencies

```bash
pip install -r requirements.txt
```

---

## ğŸš¨ Input File

Place your input file (PDF or TXT) inside:

```
data/input/
```

Example:

```
data/input/amanah_bank_policy.pdf
```

---

## â–¶ Running the Pipeline

Run via CLI:

```bash
python -m cli run \
    --input data/input/amanah_bank_policy.pdf \
    --output data/output/results.json
```

This command will:

1. Extract document text
2. Clean and normalize it
3. Chunk it into overlapping windows
4. Generate Q/A pairs per chunk
5. Extract supporting evidence sentences
6. Deduplicate similar questions
7. Save results as JSON

---

## âš™ Configuration

Edit these files to customize the pipeline:

```
config/pipeline_config.yaml   # chunk size, Q/A limits, dedupe threshold
config/model_config.yaml      # model name, embedding model, quantization options
```

---

## ğŸ§ª Running Tests

```bash
pytest -q
```

Tests cover:

* Cleaner
* Chunker
* Q/A parser

---

## ğŸ§  Notebooks

Use the `notebooks/` directory for:

* debugging
* exploring text chunks
* visualizing embeddings
* evaluating model output

---

## ğŸ¤ Contributing

Pull requests and improvements are welcome.
Follow standard Git branching with PR review.

---

## ğŸ“œ License

Open-source â€” free to use and modify.
