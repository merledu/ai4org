# -*- coding: utf-8 -*-
"""DataPipeline-enhanced-QA5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RcmXwMqmJEeyRDCv24eXujV6f1HeIHQ2
"""

#!/usr/bin/env python3
"""
pipeline_phi3_policy_qa.py
Improved Bank Policy -> Q/A pipeline tuned for Phi-3 Medium 14B (8-bit) with device_map="auto".

Features:
- Smaller chunks with overlap
- Dynamic Q count per chunk (0..max_q_per_chunk)
- Strict prompt enforcing policy names / section numbers
- Better policy name & numeric extraction regexes
- Sentence-level evidence extraction
- Exact + semantic deduplication of Q/A pairs
- Retry deterministic -> sampling generation
- Colab-friendly loading with 8-bit quantization and device_map="auto"
"""

import argparse
import json
import os
import re
import time
from pathlib import Path
from typing import List, Tuple, Dict

# ---- Install note (run in Colab cell or environment once) ----
!pip install -q transformers accelerate sentence-transformers PyPDF2 torch bitsandbytes faiss-cpu nltk scikit-learn tqdm autoawq

# -------------------------
# Config / Defaults
# -------------------------
# DEFAULT_MODEL = "microsoft/Phi-3-medium-128k-instruct"  # replace if you have a different HF path
DEFAULT_MODEL = "Qwen/Qwen2.5-7B-Instruct"
CHUNK_SIZE_WORDS = 120
CHUNK_OVERLAP_WORDS = 30
MAX_Q_PER_CHUNK = 5
MAX_NEW_TOKENS = 512
DETERMINISTIC_TEMP = 0.0
SAMPLING_TEMP = 0.4
EMBED_MODEL = "sentence-transformers/all-MiniLM-L6-v2"
SEMANTIC_DEDUPE_THRESHOLD = 0.88   # cosine similarity threshold for deduping questions
EVIDENCE_SENT_TOP_K = 2
SLEEP_BETWEEN_CHUNKS = 0.12
FILE_PATH = "amanah_bank_policy.pdf"

# -------------------------
# Lightweight imports (allow running without heavy libs for static parts)
# -------------------------
import nltk
nltk.download("punkt")
try:
    nltk.download("punkt_tab")
except:
    pass  # some environments don't require it

from nltk.tokenize import sent_tokenize

# -------------------------
# File loaders
# -------------------------
def is_pdf(path): return path.lower().endswith(".pdf")
def is_txt(path): return path.lower().endswith(".txt")

def extract_text_from_pdf(path: str) -> str:
    import PyPDF2
    pages = []
    with open(path, "rb") as f:
        reader = PyPDF2.PdfReader(f)
        for p in reader.pages:
            pages.append(p.extract_text() or "")
    return "\n".join(pages)

def extract_text(file_path: str) -> str:
    if is_pdf(file_path):
        return extract_text_from_pdf(file_path)
    elif is_txt(file_path):
        with open(file_path, "r", encoding="utf-8") as f:
            return f.read()
    else:
        raise ValueError("File must be .pdf or .txt")

# -------------------------
# Cleaning & chunking
# -------------------------
def clean_text(t: str) -> str:
    t = re.sub(r"\r\n?", "\n", t)
    t = re.sub(r"\n{2,}", "\n\n", t)   # keep paragraph breaks
    t = re.sub(r"[^\x00-\x7F]+", " ", t)   # remove non-ascii to reduce token issues
    t = re.sub(r"[ \t]{2,}", " ", t)
    return t.strip()

def chunk_text(text: str, chunk_size: int=CHUNK_SIZE_WORDS, overlap: int=CHUNK_OVERLAP_WORDS) -> List[str]:
    words = text.split()
    if len(words) <= chunk_size:
        return [" ".join(words)]
    chunks = []
    i = 0
    while i < len(words):
        chunk = words[i:i+chunk_size]
        chunks.append(" ".join(chunk))
        i += chunk_size - overlap
    return chunks

# -------------------------
# Sentence-level helper
# -------------------------
def sentences_from_text(text: str) -> List[str]:
    sents = sent_tokenize(text)
    sents = [s.strip() for s in sents if s.strip()]
    return sents

# -------------------------
# Model utilities - loads model in 8-bit with device_map='auto'
# -------------------------
from transformers import AutoTokenizer, AutoModelForCausalLM
from transformers import AutoTokenizer
# from awq import AutoAWQForCausalLM
from transformers import BitsAndBytesConfig


# def load_model_tokenizer(model_name=DEFAULT_MODEL):

#     # 8-bit config with CPU offload
#     bnb_config = BitsAndBytesConfig(
#         load_in_8bit=True,
#         llm_int8_threshold=6.0,
#         llm_int8_has_fp16_weight=False,
#         llm_int8_enable_fp32_cpu_offload=True
#     )

#     tok = AutoTokenizer.from_pretrained(model_name, use_fast=False, trust_remote_code=True)
#     if tok.pad_token is None:
#         tok.pad_token = tok.eos_token

#     model = AutoModelForCausalLM.from_pretrained(
#         model_name,
#         quantization_config=bnb_config,
#         device_map="auto",             # THIS TIME it will correctly offload
#         trust_remote_code=True
#     )

#     model.config.pad_token_id = tok.eos_token_id
#     return tok, model


def load_model_tokenizer(model_name=DEFAULT_MODEL):

    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype="float16",
    )

    tok = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)

    if tok.pad_token is None:
        tok.pad_token = tok.eos_token

    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True
    )

    model.config.pad_token_id = tok.eos_token_id
    return tok, model


# def load_model_tokenizer(model_name=DEFAULT_MODEL):

#     tok = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
#     if tok.pad_token is None:
#         tok.pad_token = tok.eos_token

#     model = AutoAWQForCausalLM.from_pretrained(
#         model_name,
#         device_map="auto",
#         trust_remote_code=True
#     )

#     model.config.pad_token_id = tok.eos_token_id
#     return tok, model

# -------------------------
# Prompt building (strict)
# -------------------------


def build_prompt(passage: str, max_q: int=MAX_Q_PER_CHUNK) -> str:
    PROMPT_TEMPLATE = f"""
    You are an expert banking compliance analyst. From the passage below, produce between 0 and {max_q} very specific Q&A pairs that refer *explicitly* to policy names or section/clause numbers present in the passage.

    REQUIREMENTS:
    - Generate ONLY questions that include a policy name or a section/clause number (e.g., "Section 2.3 Customer Risk Assessment and Profiling Policy" or "Section 5.1 Savings Account Interest Policy").
    - Do NOT produce vague questions (e.g., "What is the purpose of this policy?") unless the policy name or number is explicitly mentioned in the question.
    - Each answer must be 1–3 sentences, directly supported by the passage (do not invent facts).
    - Output EXACTLY in this format (no extra commentary):

    Q1: <question>?
    A1: <answer>

    Q2: <question>?
    A2: <answer>

    ... up to Q{max_q}

    Passage:
    {passage}
    """
    return PROMPT_TEMPLATE.format(max_q=max_q, passage=passage)

# -------------------------
# Generation + retry
# -------------------------
def generate_with_retry(tokenizer, model, prompt: str, max_new_tokens: int = MAX_NEW_TOKENS) -> str:
    import torch
    # Tokenize
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=tokenizer.model_max_length).to(model.device)
    # deterministic first
    try:
        with torch.no_grad():
            out = model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                temperature=DETERMINISTIC_TEMP,
                top_p=0.95,
                do_sample=False,
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id,
            )
        text = tokenizer.decode(out[0][inputs["input_ids"].shape[1]:], skip_special_tokens=True).strip()
    except Exception:
        text = ""

    # if empty or clearly invalid, retry sampling
    if not text or len(text) < 10:
        try:
            with torch.no_grad():
                out = model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    temperature=SAMPLING_TEMP,
                    top_p=0.95,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id,
                    eos_token_id=tokenizer.eos_token_id,
                )
            text = tokenizer.decode(out[0][inputs["input_ids"].shape[1]:], skip_special_tokens=True).strip()
        except Exception:
            text = ""

    return text

# -------------------------
# Parse Q/A blocks
# -------------------------
Q_RE = re.compile(r'^\s*Q\s*(\d+)\s*[:.\-]\s*(.+?)(\?)?\s*$', re.IGNORECASE)
A_RE = re.compile(r'^\s*A\s*(\d+)\s*[:.\-]\s*(.+)', re.IGNORECASE)

def parse_qa_block(block: str) -> List[Tuple[str,str]]:
    lines = [ln.strip() for ln in block.splitlines() if ln.strip()]
    pairs = []
    i = 0
    while i < len(lines)-1:
        q_match = Q_RE.match(lines[i])
        a_match = A_RE.match(lines[i+1])
        if q_match and a_match and q_match.group(1) == a_match.group(1):
            q = q_match.group(2).strip()
            a = a_match.group(2).strip().rstrip(".")  # normalize
            pairs.append((q, a))
            i += 2
        else:
            i += 1
    return pairs

# -------------------------
# Policy name / section detection (stronger)
# -------------------------
POLICY_NUM_RE = re.compile(r'\b(?:section|sec|clause|article|policy)\s*\d+(?:\.\d+)*\b', re.IGNORECASE)
# Named policy pattern: capitalized words ending with 'Policy' or typical nouns
NAMED_POLICY_RE = re.compile(r'\b([A-Z][A-Za-z&\-]{2,}(?:\s+[A-Z][A-Za-z&\-]{2,}){0,6}\s+(?:Policy|policy))\b')
ABBR_POLICY_RE = re.compile(r'\b(KYC|CDD|EDD|AML|CFT|FD|ATM|STR|CTR)\b', re.IGNORECASE)

def question_has_valid_reference(q: str) -> bool:
    """Return True if question mentions numeric section or a named policy / common policy acronyms."""
    if POLICY_NUM_RE.search(q):
        return True
    if NAMED_POLICY_RE.search(q):
        return True
    if ABBR_POLICY_RE.search(q):
        return True
    return False

def is_vague_question(q: str) -> bool:
    ql = q.lower()
    vague_phrases = ["purpose of this policy", "what is this policy", "what is the purpose of this policy", "what does this policy say"]
    return any(p in ql for p in vague_phrases)

def valid_question(q: str) -> bool:
    if len(q) < 5:
        return False
    if is_vague_question(q) and not question_has_valid_reference(q):
        return False
    return question_has_valid_reference(q)

# -------------------------
# Evidence extraction: choose top sentence(s) from chunk that support the answer
# -------------------------
from sentence_transformers import SentenceTransformer, util
embed_model = None

def ensure_embed_model():
    global embed_model
    if embed_model is None:
        embed_model = SentenceTransformer(EMBED_MODEL)

def extract_evidence_sentences(answer: str, chunk: str, k: int=EVIDENCE_SENT_TOP_K) -> List[str]:
    """
    Return top-k sentences from chunk most similar to answer.
    """
    sents = sentences_from_text(chunk)
    if not sents:
        return []
    ensure_embed_model()
    # embed answer and sentences
    embeddings = embed_model.encode([answer] + sents, convert_to_tensor=True)
    ans_emb = embeddings[0]
    sent_embs = embeddings[1:]
    scores = util.cos_sim(ans_emb, sent_embs)[0].cpu().numpy()
    # pick top-k
    idxs = scores.argsort()[::-1][:k]
    chosen = [sents[i] for i in idxs if scores[i] > 0.1]  # small floor
    return chosen

# -------------------------
# Semantic dedupe for questions (embeddings + threshold)
# -------------------------
def semantic_dedupe(qas: List[Dict], threshold: float=SEMANTIC_DEDUPE_THRESHOLD) -> List[Dict]:
    """
    Remove semantically similar questions. Keeps the first occurrence in case of duplicates.
    """
    if not qas:
        return []
    ensure_embed_model()
    questions = [entry["question"] for entry in qas]
    q_embs = embed_model.encode(questions, convert_to_tensor=True)
    keep = []
    used = [False]*len(questions)
    for i in range(len(questions)):
        if used[i]:
            continue
        keep.append(qas[i])
        sims = util.cos_sim(q_embs[i], q_embs).cpu().numpy()[0]
        # mark all semantically close ones
        for j in range(i+1, len(questions)):
            if sims[j] >= threshold:
                used[j] = True
    return keep

# -------------------------
# Main pipeline runner
# -------------------------
def run_pipeline(input_path: str, out_file: str):
    print("[INFO] Loading text...")
    raw = extract_text(input_path)
    print(f"[INFO] Raw text length (chars): {len(raw)}")
    cleaned = clean_text(raw)
    # Save cleaned corpus for audit
    with open("cleaned_bank_corpus.txt", "w", encoding="utf-8") as f:
        f.write(cleaned)
    print("[INFO] Saved cleaned_bank_corpus.txt")

    # Build sentence map for evidence extraction if needed
    full_sentences = sentences_from_text(cleaned)

    # Chunking
    chunks = chunk_text(cleaned, CHUNK_SIZE_WORDS, CHUNK_OVERLAP_WORDS)
    print(f"[INFO] Total chunks: {len(chunks)} (chunk size {CHUNK_SIZE_WORDS} words, overlap {CHUNK_OVERLAP_WORDS})")

    # Load model & tokenizer
    print("[INFO] Loading model and tokenizer (8-bit, device_map='auto') — this may take a minute...")
    tokenizer, model = load_model_tokenizer(DEFAULT_MODEL)
    print("[INFO] Model loaded.")


    results = []
    seen_exact = set()  # exact text dedupe for speed

    for idx, chunk in enumerate(chunks[:40]):
        prompt = build_prompt(chunk, max_q=MAX_Q_PER_CHUNK)
        text = generate_with_retry(tokenizer, model, prompt)
        if not text:
            print(f"[WARN] Empty model output for chunk {idx}")
            continue
        parsed_pairs = parse_qa_block(text)
        if not parsed_pairs:
            # no Q/A found — skip
            time.sleep(SLEEP_BETWEEN_CHUNKS)
            continue

        for q, a in parsed_pairs:
            # Normalize question text (strip trailing punctuation)
            q_norm = q.strip().rstrip("?").strip()
            a_norm = a.strip()

            # validation
            if not valid_question(q_norm):
                continue

            # exact dedupe
            key = (q_norm.lower(), a_norm.lower())
            if key in seen_exact:
                continue
            seen_exact.add(key)

            # evidence extraction (top sentences)
            evidences = extract_evidence_sentences(a_norm, chunk, k=EVIDENCE_SENT_TOP_K)

            results.append({
                "question": q_norm,
                "answer": a_norm,
                "supporting_passages": evidences if evidences else [chunk]
            })

        time.sleep(SLEEP_BETWEEN_CHUNKS)

    print(f"[INFO] Raw generated Q/A count before semantic dedupe: {len(results)}")

    # Semantic dedupe (embedding-based) to remove near-duplicates
    results = semantic_dedupe(results, threshold=SEMANTIC_DEDUPE_THRESHOLD)
    print(f"[INFO] Q/A count after semantic dedupe: {len(results)}")

    # Final save
    with open(out_file, "w", encoding="utf-8") as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    print(f"[INFO] Saved final dataset -> {out_file}")
    return results

# if __name__ == "__main__":
#     main()


run_pipeline(FILE_PATH, "results.json")