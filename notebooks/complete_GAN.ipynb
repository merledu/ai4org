{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "ljbAulhVGARs",
        "outputId": "ba193427-41ad-4f0c-bbcc-13edb9ca843f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faker in /usr/local/lib/python3.11/dist-packages (37.4.2)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.11/dist-packages (from faker) (2025.2)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_0003a2d0-bb99-417c-877f-3e3f5c3a2243\", \"english_sentences_5k.txt\", 359897)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Install faker if not already installed\n",
        "!pip install faker\n",
        "\n",
        "from faker import Faker\n",
        "import random\n",
        "\n",
        "# Initialize faker with English locale\n",
        "fake = Faker('en_US')\n",
        "Faker.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Helper function to generate a more logical sentence\n",
        "def generate_logical_sentence():\n",
        "    name = fake.name()\n",
        "    job = fake.job()\n",
        "    company = fake.company()\n",
        "    city = fake.city()\n",
        "    action = random.choice([\n",
        "        f\"started working as a {job} at {company}.\",\n",
        "        f\"moved to {city} for a new opportunity.\",\n",
        "        f\"is currently employed at {company} as a {job}.\",\n",
        "        f\"gave a presentation on {fake.catch_phrase().lower()}.\",\n",
        "        f\"attended a conference in {city}.\",\n",
        "        f\"recently published a report titled '{fake.bs().capitalize()}'.\"\n",
        "    ])\n",
        "    return f\"{name} {action}\"\n",
        "\n",
        "# Generate 5,000 sentences\n",
        "sentences = [generate_logical_sentence() for _ in range(5000)]\n",
        "\n",
        "# Save to file\n",
        "filename = \"english_sentences_5k.txt\"\n",
        "with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "    for sentence in sentences:\n",
        "        f.write(sentence + \"\\n\")\n",
        "\n",
        "# Download in Colab\n",
        "from google.colab import files\n",
        "files.download(filename)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n79xpL3CGEq2",
        "outputId": "e67b2773-27cc-41dc-c91b-f58b672ddd94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pretraining Generator...\n",
            "Generator Pretrain Loss [1]: 2.9496\n",
            "Generator Pretrain Loss [2]: 2.0163\n",
            "Generator Pretrain Loss [3]: 1.8391\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import random\n",
        "from pathlib import Path\n",
        "\n",
        "# Dataset Preparation\n",
        "file_path = Path(\"english_sentences_5k.txt\")\n",
        "with file_path.open(encoding=\"utf-8\") as f:\n",
        "    sample_lines = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "lines = [sent.lower().split() for sent in sample_lines]\n",
        "vocab = set(word for line in lines for word in line)\n",
        "word2idx = {w: i + 2 for i, w in enumerate(sorted(vocab))}\n",
        "word2idx[\"<pad>\"] = 0\n",
        "word2idx[\"<start>\"] = 1\n",
        "idx2word = {i: w for w, i in word2idx.items()}\n",
        "\n",
        "vocab_size = len(word2idx)\n",
        "max_len = max(len(l) for l in lines) + 1\n",
        "\n",
        "def encode(line):\n",
        "    return [word2idx['<start>']] + [word2idx[w] for w in line]\n",
        "\n",
        "encoded = [encode(line) + [0] * (max_len - len(line) - 1) for line in lines]\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = torch.tensor(data, dtype=torch.long)\n",
        "    def __len__(self): return len(self.data)\n",
        "    def __getitem__(self, i): return self.data[i]\n",
        "\n",
        "train_loader = DataLoader(TextDataset(encoded), batch_size=4, shuffle=True)\n",
        "\n",
        "# Generator\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim=32, hidden_dim=64, max_len=10):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(emb_dim, hidden_dim, batch_first=True)\n",
        "        self.fc_out = nn.Linear(hidden_dim, vocab_size)\n",
        "        self.vocab_size = vocab_size\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        emb = self.embedding(x)\n",
        "        out, hidden = self.lstm(emb, hidden)\n",
        "        logits = self.fc_out(out)\n",
        "        return logits, hidden\n",
        "\n",
        "    def sample(self, batch_size, device='cpu', start_token=1):\n",
        "        x = torch.full((batch_size, 1), start_token, dtype=torch.long).to(device)\n",
        "        samples = [x]\n",
        "        hidden = None\n",
        "        for _ in range(self.max_len - 1):\n",
        "            logits, hidden = self.forward(x, hidden)\n",
        "            prob = F.softmax(logits[:, -1, :], dim=-1)\n",
        "            next_token = torch.multinomial(prob, num_samples=1)\n",
        "            samples.append(next_token)\n",
        "            x = next_token\n",
        "        return torch.cat(samples, dim=1)\n",
        "\n",
        "# Discriminator\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim=32, num_filters=64, filter_sizes=[2, 3, 4], dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv2d(1, num_filters, (fs, emb_dim)) for fs in filter_sizes\n",
        "        ])\n",
        "        self.fc = nn.Linear(num_filters * len(filter_sizes), 1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x).unsqueeze(1)\n",
        "        convs = [F.relu(conv(x)).squeeze(3) for conv in self.convs]\n",
        "        pools = [F.max_pool1d(c, c.size(2)).squeeze(2) for c in convs]\n",
        "        out = self.dropout(torch.cat(pools, dim=1))\n",
        "        return self.sigmoid(self.fc(out))\n",
        "\n",
        "# Pretraining\n",
        "def pretrain_generator(generator, dataloader, criterion, optimizer, num_epochs=10, device='cpu'):\n",
        "    generator.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        for batch in dataloader:\n",
        "            batch = batch.to(device)\n",
        "            inputs = batch[:, :-1]\n",
        "            targets = batch[:, 1:]\n",
        "            logits, _ = generator(inputs)\n",
        "            loss = criterion(logits.view(-1, generator.vocab_size), targets.reshape(-1))\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Generator Pretrain Loss [{epoch+1}]: {total_loss / len(dataloader):.4f}\")\n",
        "\n",
        "def pretrain_discriminator(discriminator, generator, dataloader, criterion, optimizer, num_epochs=10, device='cpu'):\n",
        "    discriminator.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        for real_batch in dataloader:\n",
        "            real_batch = real_batch.to(device)\n",
        "            fake_batch = generator.sample(real_batch.size(0), device=device)\n",
        "            all_data = torch.cat([real_batch, fake_batch], dim=0)\n",
        "            labels = torch.cat([\n",
        "                torch.ones(real_batch.size(0), 1),\n",
        "                torch.zeros(fake_batch.size(0), 1)\n",
        "            ]).to(device)\n",
        "            pred = discriminator(all_data)\n",
        "            loss = criterion(pred, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Discriminator Pretrain Loss [{epoch+1}]: {total_loss / len(dataloader):.4f}\")\n",
        "\n",
        "# Monte Carlo Rollout\n",
        "class Rollout:\n",
        "    def __init__(self, generator, update_rate=0.8):\n",
        "        self.generator = generator\n",
        "        self.device = next(generator.parameters()).device\n",
        "        self.aux_generator = Generator(\n",
        "            generator.vocab_size,\n",
        "            generator.embedding.embedding_dim,\n",
        "            generator.lstm.hidden_size,\n",
        "            generator.max_len\n",
        "        ).to(self.device)\n",
        "        self.aux_generator.load_state_dict(generator.state_dict())\n",
        "        self.update_rate = update_rate\n",
        "\n",
        "    def get_reward(self, partial_seqs, rollout_num, discriminator, device='cpu'):\n",
        "        rewards = []\n",
        "        batch_size, seq_len = partial_seqs.size()\n",
        "        partial_seqs = partial_seqs.to(self.device)\n",
        "        for t in range(1, seq_len):\n",
        "            samples = []\n",
        "            for _ in range(rollout_num):\n",
        "                samples_t = self.rollout_step(partial_seqs[:, :t], t)\n",
        "                samples.append(samples_t)\n",
        "            samples = torch.cat(samples, dim=0)\n",
        "            scores = discriminator(samples).view(rollout_num, batch_size)\n",
        "            avg_scores = torch.mean(scores, dim=0)\n",
        "            rewards.append(avg_scores)\n",
        "        return torch.stack(rewards, dim=1).detach()\n",
        "\n",
        "    def rollout_step(self, partial_seq, t):\n",
        "        self.aux_generator.eval()\n",
        "        samples = partial_seq.to(self.device)\n",
        "        with torch.no_grad():\n",
        "            hidden = None\n",
        "            for _ in range(self.aux_generator.max_len - t):\n",
        "                logits, hidden = self.aux_generator(samples, hidden)\n",
        "                prob = F.softmax(logits[:, -1, :], dim=-1)\n",
        "                next_token = torch.multinomial(prob, num_samples=1)\n",
        "                samples = torch.cat([samples, next_token], dim=1)\n",
        "        return samples\n",
        "\n",
        "    def update_params(self):\n",
        "        for target_param, source_param in zip(self.aux_generator.parameters(), self.generator.parameters()):\n",
        "            target_param.data = self.update_rate * target_param.data + (1 - self.update_rate) * source_param.data\n",
        "\n",
        "# Policy Gradient Loss\n",
        "def generator_pg_loss(generator, samples, rewards, device='cpu'):\n",
        "    inputs = samples[:, :-1]\n",
        "    targets = samples[:, 1:]\n",
        "    logits, _ = generator(inputs.to(device))\n",
        "    log_probs = F.log_softmax(logits, dim=-1)\n",
        "    log_probs = log_probs.gather(2, targets.unsqueeze(2).to(device)).squeeze(2)\n",
        "    return -torch.mean(log_probs * rewards.to(device))\n",
        "\n",
        "def update_generator_with_pg(generator, samples, rewards, optimizer, device='cpu'):\n",
        "    generator.train()\n",
        "    loss = generator_pg_loss(generator, samples, rewards, device)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "# Adversarial Training\n",
        "def adversarial_train(generator, discriminator, rollout, dataloader, gen_optimizer, disc_optimizer, g_steps=1, d_steps=3, rollout_num=16, num_epochs=30, device='cpu'):\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Adversarial Epoch {epoch+1}\")\n",
        "        for _ in range(g_steps):\n",
        "            samples = generator.sample(batch_size=4, device=device)\n",
        "            rewards = rollout.get_reward(samples, rollout_num, discriminator, device=device)\n",
        "            loss = update_generator_with_pg(generator, samples, rewards, gen_optimizer, device)\n",
        "            print(f\"  Generator RL Loss: {loss:.4f}\")\n",
        "        rollout.update_params()\n",
        "        for _ in range(d_steps):\n",
        "            for real_batch in dataloader:\n",
        "                real_batch = real_batch.to(device)\n",
        "                fake_batch = generator.sample(real_batch.size(0), device=device)\n",
        "                all_data = torch.cat([real_batch, fake_batch], dim=0)\n",
        "                labels = torch.cat([\n",
        "                    torch.ones(real_batch.size(0), 1),\n",
        "                    torch.zeros(fake_batch.size(0), 1)\n",
        "                ]).to(device)\n",
        "                pred = discriminator(all_data)\n",
        "                d_loss = F.binary_cross_entropy(pred, labels)\n",
        "                disc_optimizer.zero_grad()\n",
        "                d_loss.backward()\n",
        "                disc_optimizer.step()\n",
        "        print(f\"  Discriminator Loss: {d_loss.item():.4f}\")\n",
        "\n",
        "# Run\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "gen = Generator(vocab_size, max_len=max_len).to(device)\n",
        "disc = Discriminator(vocab_size).to(device)\n",
        "gen_opt = torch.optim.Adam(gen.parameters(), lr=1e-3)\n",
        "disc_opt = torch.optim.Adam(disc.parameters(), lr=1e-4)\n",
        "\n",
        "print(\"Pretraining Generator...\")\n",
        "pretrain_generator(gen, train_loader, nn.CrossEntropyLoss(), gen_opt, num_epochs=10, device=device)\n",
        "\n",
        "print(\"Pretraining Discriminator...\")\n",
        "pretrain_discriminator(disc, gen, train_loader, nn.BCELoss(), disc_opt, num_epochs=10, device=device)\n",
        "\n",
        "print(\"Starting Adversarial Training...\")\n",
        "rollout = Rollout(gen)\n",
        "adversarial_train(gen, disc, rollout, train_loader, gen_opt, disc_opt, num_epochs=20, device=device)\n",
        "\n",
        "# Generate samples\n",
        "gen.eval()\n",
        "samples = gen.sample(4, device=device)\n",
        "for row in samples:\n",
        "    words = []\n",
        "    for idx in row:\n",
        "        word = idx2word[idx.item()]\n",
        "        if word == '<pad>':\n",
        "            break\n",
        "        words.append(word)\n",
        "    print(\"output:\", ' '.join(words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z34zP-N9EtA6"
      },
      "outputs": [],
      "source": [
        "# to save the model for later use\n",
        "torch.save(gen.state_dict(), \"seqgan_generator.pth\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XCNNxAXEvGi"
      },
      "outputs": [],
      "source": [
        "# To load the model later\n",
        "gen_loaded = Generator(vocab_size, max_len=max_len).to(device)\n",
        "gen_loaded.load_state_dict(torch.load(\"seqgan_generator.pth\", map_location=device))\n",
        "gen_loaded.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1u2Nb1LE1Zl"
      },
      "outputs": [],
      "source": [
        "def generate_from_prompt(generator, prompt, word2idx, idx2word, max_len=20, device='cpu'):\n",
        "    generator.eval()\n",
        "    words = prompt.lower().strip().split()\n",
        "    input_ids = [word2idx.get(w, word2idx['<pad>']) for w in words]\n",
        "    input_tensor = torch.tensor([[word2idx['<start>']] + input_ids], dtype=torch.long).to(device)\n",
        "\n",
        "    hidden = None\n",
        "    output_seq = input_tensor\n",
        "    for _ in range(max_len - input_tensor.size(1)):\n",
        "        logits, hidden = generator(output_seq, hidden)\n",
        "        probs = F.softmax(logits[:, -1, :], dim=-1)\n",
        "        next_token = torch.multinomial(probs, num_samples=1)\n",
        "        output_seq = torch.cat([output_seq, next_token], dim=1)\n",
        "        if next_token.item() == word2idx['<pad>']:\n",
        "            break\n",
        "\n",
        "    output_words = [idx2word[idx.item()] for idx in output_seq[0] if idx.item() > 1]\n",
        "    return ' '.join(output_words)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oD0hbj1hFG7H"
      },
      "outputs": [],
      "source": [
        "prompt = \"Allison Hill\"\n",
        "generated_text = generate_from_prompt(gen, prompt, word2idx, idx2word, max_len=20, device=device)\n",
        "print(\"Generated:\", generated_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aa6EQ6ZVJ5rb"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
        "\n",
        "# Prepare reference and hypothesis sentences\n",
        "smoothie = SmoothingFunction().method4\n",
        "gen.eval()\n",
        "generated = gen.sample(4, device=device)\n",
        "\n",
        "references = [[line] for line in lines[:4]]  # List of lists of tokens\n",
        "hypotheses = []\n",
        "for row in generated:\n",
        "    sentence = []\n",
        "    for idx in row:\n",
        "        word = idx2word[idx.item()]\n",
        "        if word == '<pad>':\n",
        "            break\n",
        "        sentence.append(word)\n",
        "    hypotheses.append(sentence)\n",
        "\n",
        "# Compute final BLEU score using corpus_bleu\n",
        "final_bleu = corpus_bleu(references, hypotheses, smoothing_function=smoothie)\n",
        "print(f\"Final BLEU Score of Generator: {final_bleu:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikR_sAeB8lZj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "# Get real and fake batches\n",
        "real_batch = next(iter(train_loader)).to(device)\n",
        "fake_batch = gen.sample(real_batch.size(0), device=device)\n",
        "\n",
        "# True labels: 1 for real, 0 for fake\n",
        "y_true = torch.cat([\n",
        "    torch.ones(real_batch.size(0)),   # real\n",
        "    torch.zeros(fake_batch.size(0))   # fake\n",
        "]).to(device)\n",
        "\n",
        "# Get discriminator predictions (rounded to 0 or 1)\n",
        "disc.eval()\n",
        "with torch.no_grad():\n",
        "    all_data = torch.cat([real_batch, fake_batch], dim=0)\n",
        "    y_pred = disc(all_data).squeeze().round()\n",
        "\n",
        "# Convert to CPU for sklearn\n",
        "y_true_np = y_true.cpu().numpy()\n",
        "y_pred_np = y_pred.cpu().numpy()\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_true_np, y_pred_np)\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "# Print matrix with labels\n",
        "print(\"Confusion Matrix:\")\n",
        "print(f\"               Predicted Fake   Predicted Real\")\n",
        "print(f\"Actual Fake       {tn}               {fp}\")\n",
        "print(f\"Actual Real       {fn}               {tp}\")\n",
        "\n",
        "# Compute metrics\n",
        "accuracy  = accuracy_score(y_true_np, y_pred_np)\n",
        "precision = precision_score(y_true_np, y_pred_np)\n",
        "recall    = recall_score(y_true_np, y_pred_np)\n",
        "f1        = f1_score(y_true_np, y_pred_np)\n",
        "\n",
        "print(f\"\\nAccuracy:  {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall:    {recall:.2f}\")\n",
        "print(f\"F1 Score:  {f1:.2f}\")\n",
        "\n",
        "# Plot confusion matrix\n",
        "labels = ['Fake', 'Real']\n",
        "fig, ax = plt.subplots(figsize=(5, 5))\n",
        "im = ax.imshow(cm, cmap='viridis')\n",
        "ax.set_title(\"Discriminator Confusion Matrix\")\n",
        "ax.set_xticks([0, 1])\n",
        "ax.set_yticks([0, 1])\n",
        "ax.set_xticklabels(labels)\n",
        "ax.set_yticklabels(labels)\n",
        "ax.set_xlabel(\"Predicted Label\")\n",
        "ax.set_ylabel(\"True Label\")\n",
        "\n",
        "# Annotate matrix cells\n",
        "for i in range(2):\n",
        "    for j in range(2):\n",
        "        ax.text(j, i, cm[i, j], ha='center', va='center', color='white' if cm[i,j]>1 else 'black')\n",
        "\n",
        "plt.colorbar(im, ax=ax)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2OyBi6r-Wnt"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}